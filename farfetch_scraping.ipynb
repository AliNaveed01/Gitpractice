{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Product URLs From Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 11 product URLs on page 1\n",
      "Scraping page 2...\n",
      "Found 12 product URLs on page 2\n",
      "Scraping page 3...\n",
      "Found 12 product URLs on page 3\n",
      "Scraping page 4...\n",
      "Found 12 product URLs on page 4\n",
      "Scraping page 5...\n",
      "Found 12 product URLs on page 5\n",
      "Scraping page 6...\n",
      "Found 12 product URLs on page 6\n",
      "Scraping page 7...\n",
      "Found 12 product URLs on page 7\n",
      "Scraping page 8...\n",
      "Found 12 product URLs on page 8\n",
      "Scraping page 9...\n",
      "Found 12 product URLs on page 9\n",
      "Scraping page 10...\n",
      "Found 12 product URLs on page 10\n",
      "Scraping page 11...\n",
      "Found 12 product URLs on page 11\n",
      "Scraping page 12...\n",
      "Found 12 product URLs on page 12\n",
      "Scraping page 13...\n",
      "Found 12 product URLs on page 13\n",
      "Scraping page 14...\n",
      "Found 12 product URLs on page 14\n",
      "Scraping page 15...\n",
      "Found 12 product URLs on page 15\n",
      "Scraping page 16...\n",
      "Found 12 product URLs on page 16\n",
      "Scraping page 17...\n",
      "Found 12 product URLs on page 17\n",
      "Scraping page 18...\n",
      "Found 12 product URLs on page 18\n",
      "Scraping page 19...\n",
      "Found 12 product URLs on page 19\n",
      "Scraping page 20...\n",
      "Found 12 product URLs on page 20\n",
      "Scraping page 21...\n",
      "Found 12 product URLs on page 21\n",
      "Scraping page 22...\n",
      "Found 12 product URLs on page 22\n",
      "Scraping page 23...\n",
      "Found 12 product URLs on page 23\n",
      "Scraping page 24...\n",
      "Found 12 product URLs on page 24\n",
      "Scraping page 25...\n",
      "Found 12 product URLs on page 25\n",
      "Scraping page 26...\n",
      "Found 12 product URLs on page 26\n",
      "Scraping page 27...\n",
      "Found 12 product URLs on page 27\n",
      "Scraping page 28...\n",
      "Found 12 product URLs on page 28\n",
      "Scraping page 29...\n",
      "Found 12 product URLs on page 29\n",
      "Scraping page 30...\n",
      "Found 12 product URLs on page 30\n",
      "Scraping page 31...\n",
      "Found 12 product URLs on page 31\n",
      "Scraping page 32...\n",
      "Found 12 product URLs on page 32\n",
      "Scraping page 33...\n",
      "Found 12 product URLs on page 33\n",
      "Scraping page 34...\n",
      "Found 12 product URLs on page 34\n",
      "Scraping page 35...\n",
      "Found 12 product URLs on page 35\n",
      "Scraping page 36...\n",
      "Found 12 product URLs on page 36\n",
      "Scraping page 37...\n",
      "Found 12 product URLs on page 37\n",
      "Scraping page 38...\n",
      "Found 12 product URLs on page 38\n",
      "Scraping page 39...\n",
      "Found 12 product URLs on page 39\n",
      "Scraping page 40...\n",
      "Found 12 product URLs on page 40\n",
      "Scraping page 41...\n",
      "Found 12 product URLs on page 41\n",
      "Scraping page 42...\n",
      "Found 12 product URLs on page 42\n",
      "Scraping page 43...\n",
      "Found 12 product URLs on page 43\n",
      "Scraping page 44...\n",
      "Found 12 product URLs on page 44\n",
      "Scraping page 45...\n",
      "Found 12 product URLs on page 45\n",
      "Scraping page 46...\n",
      "Found 12 product URLs on page 46\n",
      "Scraping page 47...\n",
      "Found 12 product URLs on page 47\n",
      "Scraping page 48...\n",
      "Found 12 product URLs on page 48\n",
      "Scraping page 49...\n",
      "Found 12 product URLs on page 49\n",
      "Scraping page 50...\n",
      "Found 12 product URLs on page 50\n",
      "Scraping page 51...\n",
      "Found 12 product URLs on page 51\n",
      "Scraping page 52...\n",
      "Found 12 product URLs on page 52\n",
      "Scraping page 53...\n",
      "Found 12 product URLs on page 53\n",
      "Scraping page 54...\n",
      "Found 12 product URLs on page 54\n",
      "Scraping page 55...\n",
      "Found 12 product URLs on page 55\n",
      "Scraping page 56...\n",
      "Found 12 product URLs on page 56\n",
      "Scraping page 57...\n",
      "Found 12 product URLs on page 57\n",
      "Scraping page 58...\n",
      "Found 12 product URLs on page 58\n",
      "Scraping page 59...\n",
      "Found 12 product URLs on page 59\n",
      "Scraping page 60...\n",
      "Found 12 product URLs on page 60\n",
      "Scraping page 61...\n",
      "Found 12 product URLs on page 61\n",
      "Scraping page 62...\n",
      "Found 0 product URLs on page 62\n",
      "Scraping page 63...\n",
      "Found 12 product URLs on page 63\n",
      "Scraping page 64...\n",
      "Found 12 product URLs on page 64\n",
      "Scraping page 65...\n",
      "Found 12 product URLs on page 65\n",
      "Scraping page 66...\n",
      "Found 12 product URLs on page 66\n",
      "Scraping page 67...\n",
      "Found 12 product URLs on page 67\n",
      "Scraping page 68...\n",
      "Found 0 product URLs on page 68\n",
      "Scraping page 69...\n",
      "Found 12 product URLs on page 69\n",
      "Scraping page 70...\n",
      "Found 0 product URLs on page 70\n",
      "Scraping page 71...\n",
      "Found 12 product URLs on page 71\n",
      "Scraping page 72...\n",
      "Found 0 product URLs on page 72\n",
      "Scraping page 73...\n",
      "Found 12 product URLs on page 73\n",
      "Scraping page 74...\n",
      "Found 12 product URLs on page 74\n",
      "Scraping page 75...\n",
      "Found 12 product URLs on page 75\n",
      "Scraping page 76...\n",
      "Found 12 product URLs on page 76\n",
      "Scraping page 77...\n",
      "Found 12 product URLs on page 77\n",
      "Scraping page 78...\n",
      "Found 12 product URLs on page 78\n",
      "Scraping page 79...\n",
      "Found 12 product URLs on page 79\n",
      "Scraping page 80...\n",
      "Found 12 product URLs on page 80\n",
      "Scraping page 81...\n",
      "Found 0 product URLs on page 81\n",
      "Scraping page 82...\n",
      "Found 12 product URLs on page 82\n",
      "Scraping page 83...\n",
      "Found 12 product URLs on page 83\n",
      "Scraping page 84...\n",
      "Found 0 product URLs on page 84\n",
      "Scraping page 85...\n",
      "Found 0 product URLs on page 85\n",
      "Scraping page 86...\n",
      "Found 0 product URLs on page 86\n",
      "Scraping page 87...\n",
      "Found 0 product URLs on page 87\n",
      "Scraping page 88...\n",
      "Found 12 product URLs on page 88\n",
      "Scraping page 89...\n",
      "Found 12 product URLs on page 89\n",
      "Scraping page 90...\n",
      "Found 0 product URLs on page 90\n",
      "Scraping page 91...\n",
      "Found 0 product URLs on page 91\n",
      "Scraping page 92...\n",
      "Found 0 product URLs on page 92\n",
      "Scraping page 93...\n",
      "Found 0 product URLs on page 93\n",
      "Scraping page 94...\n",
      "Found 0 product URLs on page 94\n",
      "Scraping page 95...\n",
      "Found 0 product URLs on page 95\n",
      "Scraping page 96...\n",
      "Found 0 product URLs on page 96\n",
      "Scraping page 97...\n",
      "Found 0 product URLs on page 97\n",
      "Scraping page 98...\n",
      "Found 0 product URLs on page 98\n",
      "Scraping page 99...\n",
      "Found 0 product URLs on page 99\n",
      "Scraping page 100...\n",
      "Found 0 product URLs on page 100\n",
      "Scraping page 101...\n",
      "Found 0 product URLs on page 101\n",
      "Scraping page 102...\n",
      "Found 0 product URLs on page 102\n",
      "Scraping page 103...\n",
      "Found 12 product URLs on page 103\n",
      "Scraping page 104...\n",
      "Found 0 product URLs on page 104\n",
      "Scraping page 105...\n",
      "Found 0 product URLs on page 105\n",
      "Scraping page 106...\n",
      "Found 0 product URLs on page 106\n",
      "Scraping page 107...\n",
      "Found 0 product URLs on page 107\n",
      "Total Products Found: 971\n"
     ]
    }
   ],
   "source": [
    "no_of_pages = 107  # Set the number of pages you want to scrape\n",
    "base_url = \"https://www.farfetch.com/pk/shopping/women/jackets-1/items.aspx?page=\"\n",
    "\n",
    "product_urls = []\n",
    "\n",
    "\n",
    "# Iterate over each page\n",
    "for page in range(1, no_of_pages + 1):\n",
    "    url = f\"{base_url}{page}\"\n",
    "    try:\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        # # Load the page\n",
    "        # driver = webdriver.Chrome()\n",
    "        # driver.set_page_load_timeout(8)  # Set page load timeout to 8 seconds\n",
    "\n",
    "        # try:\n",
    "        #     driver.get(url)\n",
    "        # except TimeoutException:\n",
    "        #     pass  # Handle timeout exception, page will not load fully\n",
    "\n",
    "        # # Get the page source after it's loaded\n",
    "        # html = driver.page_source\n",
    "\n",
    "        # # Quit the driver after scraping\n",
    "        # driver.quit()\n",
    "\n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        # soup = BeautifulSoup(html, 'html.parser')\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:123.0) Gecko/20100101 Firefox/123.0'})\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all product links\n",
    "        lis = soup.find_all('li', attrs={'data-testid': 'productCard'})\n",
    "\n",
    "        # Append the product URLs to the list\n",
    "        urls_count = 0\n",
    "        for li in lis:\n",
    "            try:\n",
    "                product_urls.append(li.find('a')['href'])\n",
    "                urls_count += 1\n",
    "            except:\n",
    "                pass\n",
    "        print(f\"Found {urls_count} product URLs on page {page}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping page {page}: {e}\")\n",
    "\n",
    "# Create a DataFrame of the product URLs\n",
    "products_df = pd.DataFrame(product_urls, columns=['Product Links'])\n",
    "print(f\"Total Products Found: {len(products_df)}\")\n",
    "\n",
    "# Saving product links to csv\n",
    "products_df.to_csv('farfetch_women_jackets_product_pages_urls.csv', index=False)\n",
    "products_df = pd.read_csv('farfetch_women_jackets_product_pages_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/pk/shopping/women/dries-van-noten-cropped-woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/pk/shopping/women/brunello-cucinelli-sequin-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/pk/shopping/women/versace-double-breasted-twe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/pk/shopping/women/prada-single-breasted-pinst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/pk/shopping/women/moncler-lampusa-denim-hoode...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Product Links\n",
       "0  /pk/shopping/women/dries-van-noten-cropped-woo...\n",
       "1  /pk/shopping/women/brunello-cucinelli-sequin-e...\n",
       "2  /pk/shopping/women/versace-double-breasted-twe...\n",
       "3  /pk/shopping/women/prada-single-breasted-pinst...\n",
       "4  /pk/shopping/women/moncler-lampusa-denim-hoode..."
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product Links    971\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Images From Each Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IP</th>\n",
       "      <th>Port</th>\n",
       "      <th>Protocol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184.168.121.153</td>\n",
       "      <td>57421.0</td>\n",
       "      <td>SOCKS5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200.143.99.122</td>\n",
       "      <td>3128.0</td>\n",
       "      <td>HTTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125.16.181.179</td>\n",
       "      <td>9988.0</td>\n",
       "      <td>HTTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125.16.181.180</td>\n",
       "      <td>9988.0</td>\n",
       "      <td>HTTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>129.153.42.81</td>\n",
       "      <td>3128.0</td>\n",
       "      <td>HTTP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                IP     Port  Protocol \n",
       "0  184.168.121.153   57421.0   SOCKS5 \n",
       "1   200.143.99.122    3128.0     HTTP \n",
       "2   125.16.181.179    9988.0     HTTP \n",
       "3   125.16.181.180    9988.0     HTTP \n",
       "4    129.153.42.81    3128.0     HTTP "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proxies_df = pd.read_csv('proxies.csv')\n",
    "proxies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxiess = {\n",
    "   'http': 'http://184.168.121.153:57421',\n",
    "   'https': 'http://200.143.99.122:3128',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http': 'http://72.10.160.92 :1451', 'https': 'http://72.10.160.92 :1451'}\n"
     ]
    }
   ],
   "source": [
    "proxies_dict = {}\n",
    "for index, row in proxies_df.iterrows():\n",
    "    if row['Protocol '] == 'HTTP ':\n",
    "        proxies_dict['http'] = f'http://{row[\"IP \"]}:{int(row[\"Port \"])}'\n",
    "        proxies_dict['https'] = f'http://{row[\"IP \"]}:{int(row[\"Port \"])}'\n",
    "    elif row['Protocol '] == 'SOCKS5 ':\n",
    "        proxies_dict['http'] = f'socks5://{row[\"IP \"]}:{int(row[\"Port \"])}'\n",
    "        proxies_dict['https'] = f'socks5://{row[\"IP \"]}:{int(row[\"Port \"])}'\n",
    "\n",
    "print(proxies_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.farfetch.com/pk/shopping/women/dries-van-noten-cropped-wool-blend-bomber-jacket-item-22924009.aspx?storeid=15268\n"
     ]
    },
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='www.farfetch.com', port=443): Max retries exceeded with url: /pk/shopping/women/dries-van-noten-cropped-wool-blend-bomber-jacket-item-22924009.aspx?storeid=15268 (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:779\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 779\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m, SocketTimeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1048\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1042\u001b[0m conn\u001b[38;5;241m.\u001b[39mset_tunnel(\n\u001b[1;32m   1043\u001b[0m     scheme\u001b[38;5;241m=\u001b[39mtunnel_scheme,\n\u001b[1;32m   1044\u001b[0m     host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host,\n\u001b[1;32m   1045\u001b[0m     port\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport,\n\u001b[1;32m   1046\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy_headers,\n\u001b[1;32m   1047\u001b[0m )\n\u001b[0;32m-> 1048\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:633\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_connected_to_proxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tunnel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# Override the host with the one we're requesting data from.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:925\u001b[0m, in \u001b[0;36mHTTPConnection._tunnel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 925\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTunnel connection failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: Tunnel connection failed: 403 Forbidden",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;31mProxyError\u001b[0m: ('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden'))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:847\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    845\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 847\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.farfetch.com', port=443): Max retries exceeded with url: /pk/shopping/women/dries-van-noten-cropped-wool-blend-bomber-jacket-item-22924009.aspx?storeid=15268 (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m full_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.farfetch.com\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(full_url)\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUser-Agent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMozilla/5.0 (X11; Linux x86_64; Storebot-Google/1.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/W.X.Y.Z Safari/537.36\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxiess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(soup\u001b[38;5;241m.\u001b[39mprettify())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:513\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _ProxyError):\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mProxyError\u001b[0m: HTTPSConnectionPool(host='www.farfetch.com', port=443): Max retries exceeded with url: /pk/shopping/women/dries-van-noten-cropped-wool-blend-bomber-jacket-item-22924009.aspx?storeid=15268 (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "products_df = pd.read_csv('farfetch_women_jackets_product_pages_urls.csv')\n",
    "\n",
    "images_urls = []\n",
    "\n",
    "for i, url in enumerate(list(products_df['Product Links'][:10])):\n",
    "    full_url = f\"https://www.farfetch.com{url}\"\n",
    "    print(full_url)\n",
    "    response = requests.get(full_url, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; Storebot-Google/1.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/W.X.Y.Z Safari/537.36'}, proxies=proxiess)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(soup.prettify())\n",
    "    divs = soup.find_all('div', class_='ltr-bjn8wh ed0fyxo0')\n",
    "    j = 0\n",
    "    for div in divs:\n",
    "        try:\n",
    "            images_urls.append([i+1, j+1, div.find('img')['src']])\n",
    "            j += 1\n",
    "            print(f\"Fetched product no# {i+1} image no# {j+1} url.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching product no# {i+1} image no# {j+1} url: {e}\")\n",
    "    divs = None\n",
    "\n",
    "images_df = pd.DataFrame(images_urls, columns=['Product', 'Image No#', 'Product URL'])\n",
    "images_df.to_csv('farfetch_women_jackets_product_images_urls.csv', index=False)\n",
    "\n",
    "images_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download images\n",
    "import requests\n",
    "\n",
    "with open(\"image.jpg\", \"wb\") as f:\n",
    "    f.write(requests.get(images[0]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<figure class=\"pdp-image product-detail-images product-detail-main-image\"><div class=\"product-detail-main-image-container\">\n",
       " <img alt=\"Relaxed Fit Denim Jacket - Dark gray denim - Men | H&amp;M US\" height=\"1152\" sizes=\"(max-width: 767px) 100vw, 50vw\" src=\"//lp2.hm.com/hmgoepprod?set=quality%5B79%5D%2Csource%5B%2F40%2F83%2F40831c5db0080e1c83b46cdc71c8f9156681ce90.jpg%5D%2Corigin%5Bdam%5D%2Ccategory%5B%5D%2Ctype%5BLOOKBOOK%5D%2Cres%5Bm%5D%2Chmver%5B1%5D&amp;call=url[file:/product/main]\" srcset=\"//lp2.hm.com/hmgoepprod?set=quality%5B79%5D%2Csource%5B%2F40%2F83%2F40831c5db0080e1c83b46cdc71c8f9156681ce90.jpg%5D%2Corigin%5Bdam%5D%2Ccategory%5B%5D%2Ctype%5BLOOKBOOK%5D%2Cres%5Bm%5D%2Chmver%5B1%5D&amp;call=url[file:/product/main] 396w,\n",
       " \t\t//lp2.hm.com/hmgoepprod?set=quality%5B79%5D%2Csource%5B%2F40%2F83%2F40831c5db0080e1c83b46cdc71c8f9156681ce90.jpg%5D%2Corigin%5Bdam%5D%2Ccategory%5B%5D%2Ctype%5BLOOKBOOK%5D%2Cres%5Bm%5D%2Chmver%5B1%5D&amp;call=url[file:/product/main] 564w, \n",
       "         //lp2.hm.com/hmgoepprod?set=quality%5B79%5D%2Csource%5B%2F40%2F83%2F40831c5db0080e1c83b46cdc71c8f9156681ce90.jpg%5D%2Corigin%5Bdam%5D%2Ccategory%5B%5D%2Ctype%5BLOOKBOOK%5D%2Cres%5Bm%5D%2Chmver%5B1%5D&amp;call=url[file:/product/main] 657w, \n",
       "         //lp2.hm.com/hmgoepprod?set=quality%5B79%5D%2Csource%5B%2F40%2F83%2F40831c5db0080e1c83b46cdc71c8f9156681ce90.jpg%5D%2Corigin%5Bdam%5D%2Ccategory%5B%5D%2Ctype%5BLOOKBOOK%5D%2Cres%5Bm%5D%2Chmver%5B1%5D&amp;call=url[file:/product/main] 820w\" width=\"768\"/>\n",
       " </div>\n",
       " </figure>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# urlhm = \"https://www2.hm.com/en_us/productpage.1130141010.html\"\n",
    "# response = requests.get(urlhm, headers={'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:123.0) Gecko/20100101 Firefox/123.0'})\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# find everything that has class pdp-image\n",
    "imgs = soup.find_all('figure')\n",
    "imgs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
